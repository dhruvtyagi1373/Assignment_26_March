{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147abff1-e1f1-4006-afad-f1fbb242e590",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781c0b74-f806-4e17-987b-0956f4c28fee",
   "metadata": {},
   "source": [
    "-> In Simple Linear Regression there is only one independent featue and one dependent feature and the relationship between them is shown using a straight line whereas in Multiple Regression there is more than one independent feature and the relationship between them is depicted using a plane in 3d-plane.\n",
    "\n",
    "Eg.Simple Linear Regression -> Weight of a person is linearly related to its height.\n",
    "\n",
    "   Multiple Regression -> No. Of Rooms and Room Size is related to the Price of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f35ffee-ce94-4ab2-b4ab-426b938eb4bc",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ab549-6e95-4aab-bb55-d134231acafe",
   "metadata": {},
   "source": [
    "1. Linear relationship\n",
    "2. Multivariate normality\n",
    "3. No or little multicollinearity\n",
    "4. No auto-correlation\n",
    "5. Homoscedasticity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ebff9-6b1d-4596-8906-43508264f7fc",
   "metadata": {},
   "source": [
    "- Use the residual plots to check the linearity and homoscedasticity\n",
    "- Use Breusch-Pagan testing for homoscedasticity\n",
    "- Use VIF test for multicollinearity\n",
    "- Check the zero conditional mean assumption\n",
    "- Check for Omitted Variable Bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8f2abc-1788-4ac2-9104-fe0df6506bf0",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770a45ea-6b08-4e35-a58d-70b1b5406bf1",
   "metadata": {},
   "source": [
    "The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac5f42d-87ea-423e-8e16-7e39b140bb6a",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f554f2b7-127f-4ac0-be63-0809682f4e7f",
   "metadata": {},
   "source": [
    "Sol : Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff0c80d-8341-445d-a862-90232f8cdf98",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b356d658-2cee-472f-a648-97a5ca11b1d6",
   "metadata": {},
   "source": [
    "- Gradient descent (GD) is an iterative first-order optimisation algorithm used to find a local minimum/maximum of a given function. This method is commonly used in machine learning (ML) and deep learning(DL) to minimise a cost/loss function (e.g. in a linear regression).\n",
    "- It has more than one independent feature whereas in the simple linear regression there is only one dependent feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61f1263-d2ee-4571-95a7-43568f44a723",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c604b0-d38a-43ed-a495-0599cd76c9c5",
   "metadata": {},
   "source": [
    "- Multicollinearity exists whenever an independent variable is highly correlated with one or more of the other independent variables in a multiple regression equation.\n",
    "- The variance inflation factor (VIF) identifies correlation between independent variables and the strength of that correlation.\n",
    "1. Remove some of the highly correlated independent variables.\n",
    "2. Linearly combine the independent variables, such as adding them together.\n",
    "3. Partial least squares regression uses principal component analysis to create a set of uncorrelated components to include in the model.\n",
    "4. LASSO and Ridge regression are advanced forms of regression analysis that can handle multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a77c32e-105e-4928-a4dd-dfe9fe649320",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d535d50-072e-4f2a-ad6c-099cadcdb552",
   "metadata": {},
   "source": [
    "Sol : A polynomial regression model is a machine learning model that can capture non-linear relationships between variables by fitting a non-linear regression line, which may not be possible with simple linear regression. It is used when linear regression models may not adequately capture the complexity of the relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873722d-11ec-493d-a229-e4cef36d125e",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c9d17b-5c03-423c-97f0-9f294885afa8",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- The polynomial regression is flexible enough to get fitted in a vast range of curvatures. \n",
    "- A broad range of functions can easily fit under it. \n",
    "- The polynomial regression offers the best approximation of the relationship between the two dependent and independent variables. \n",
    "\n",
    "Disadvantages:\n",
    "- The presence of one or more outliers in the data can hurt the final results of the nonlinear analysis. \n",
    "- The polynomial regression is very sensitive to the outliers. \n",
    "- Very few model validation tools are available that help detect the outliers in nonlinear regression compared to the ones present for linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41b7e3e-5f16-4ff8-bf40-804290ba710f",
   "metadata": {},
   "source": [
    "1. For cases where the data points are arranged in a non-linear fashion, there is a need for polynomial regression.\n",
    "2. If a non-linear model is present and you try to cover it using a linear model, it will cover no data points. Hence, a polynomial model is used to ensure that the data points are covered. That said, a curve will be suitable for covering most data points using polynomial models instead of a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb95a0-a6ed-41d7-80fe-c430ebae4e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
